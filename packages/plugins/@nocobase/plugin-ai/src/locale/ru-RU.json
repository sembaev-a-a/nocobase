{
  "Add content": "Добавить контент",
  "Add prompt": "Добавить запрос",
  "AI integration": "Интеграция с ИИ",
  "Frequency penalty description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены в зависимости от их частоты в тексте, уменьшая вероятность повторения модели.",
  "Get models list failed, you can enter a model name manually.": "Не удалось получить список моделей, вы можете ввести имя модели вручную.",
  "Image": "Изображение",
  "LLM service": "Сервис LLM",
  "LLM services": "Сервисы LLM",
  "Max completion tokens description": "Верхняя граница количества токенов, которые могут быть сгенерированы для завершения, включая видимые выходные токены и токены рассуждений.",
  "Max retries": "Максимум попыток",
  "Message": "Сообщение",
  "Messages": "Сообщения",
  "Model": "Модель",
  "Presence penalty description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены в зависимости от их присутствия в тексте, увеличивая вероятность обсуждения новых тем.",
  "Provider": "Провайдер",
  "Response format description": "Важно: при использовании режима JSON необходимо также указать модели генерировать JSON через системное или пользовательское сообщение.",
  "Role": "Роль",
  "Structured output": "Структурированный вывод",
  "Temperature description": "Температура выборки от 0 до 2. Высокие значения (например, 0.8) делают вывод более случайным, низкие (например, 0.2) — более сфокусированным и детерминированным.",
  "Text": "Текст",
  "Timout (ms)": "Тайм-аут (мс)",
  "Top P description": "Альтернатива выборке с температурой — ядерная выборка, где модель учитывает токены с массой вероятности top_p. Например, 0.1 означает учет только 10% наиболее вероятных токенов.",
  "UID": "UID"
  }
